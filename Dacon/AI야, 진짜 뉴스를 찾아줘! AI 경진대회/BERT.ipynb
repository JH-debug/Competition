{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Bert.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1XBsV06uyu3uXlaHxlO-yYUjqRijOoiI8","authorship_tag":"ABX9TyP+HLkWaJ9fzozQzL8qYLTd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"da5ecad17dbb4effa62dcf274ecebde3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4b73658b8cad4e6eabbf75804722e566","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_871d4ad0d8de4bf480bd7f2ed1036fea","IPY_MODEL_5f0990d7ca0b46d7ac7e91a36f15c10f"]}},"4b73658b8cad4e6eabbf75804722e566":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"871d4ad0d8de4bf480bd7f2ed1036fea":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_27cf473c4fa247769e282dddad9e4ccd","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":995526,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":995526,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f89d4188df664aff984377d4fcfccc94"}},"5f0990d7ca0b46d7ac7e91a36f15c10f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_31247ce699c2414bb17ceea0732d341c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 996k/996k [00:00&lt;00:00, 7.32MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_586bced4b8db4836a6f893622e566ff5"}},"27cf473c4fa247769e282dddad9e4ccd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f89d4188df664aff984377d4fcfccc94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"31247ce699c2414bb17ceea0732d341c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"586bced4b8db4836a6f893622e566ff5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"988dbe992b8c432b80863cf7e2ea4561":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3df5e054bfcc41ad971b428cf2afdeef","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_aed37a2fc8c64f1a9064bc1b30575525","IPY_MODEL_2de7c80e0d314ba7ac1afc6048bf023e"]}},"3df5e054bfcc41ad971b428cf2afdeef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"aed37a2fc8c64f1a9064bc1b30575525":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_642a38168ccd401eb02a97631f2ae20f","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":625,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":625,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b9b041ce2cec43a7a9ea761491451a22"}},"2de7c80e0d314ba7ac1afc6048bf023e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f2b4c1018ea94ffa89c8a747b3ac7479","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 625/625 [00:00&lt;00:00, 3.29kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_863152ec127641d5acae03ff96e539cb"}},"642a38168ccd401eb02a97631f2ae20f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b9b041ce2cec43a7a9ea761491451a22":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f2b4c1018ea94ffa89c8a747b3ac7479":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"863152ec127641d5acae03ff96e539cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"886e6bd464814a28a7d6d8c977acce82":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_041cb542ecff4ae6abf1503bc2fefb39","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2ade8f6e2a5e4b99a4f387a309fefafd","IPY_MODEL_66b4e1c1cc8a4bbc9bdb60d7efab363a"]}},"041cb542ecff4ae6abf1503bc2fefb39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2ade8f6e2a5e4b99a4f387a309fefafd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_349ba1e45cf543208edbf92b163050cb","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":714314041,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":714314041,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_73f03bada66443eba3bb76a417f3cce3"}},"66b4e1c1cc8a4bbc9bdb60d7efab363a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2c0efe2be0b34424a16ef1c2dc6c86c4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 714M/714M [00:27&lt;00:00, 26.4MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f186baa789ae4f49b27e52818617fef0"}},"349ba1e45cf543208edbf92b163050cb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"73f03bada66443eba3bb76a417f3cce3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2c0efe2be0b34424a16ef1c2dc6c86c4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f186baa789ae4f49b27e52818617fef0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UsTQGd82_a8w","executionInfo":{"status":"ok","timestamp":1609376082111,"user_tz":-540,"elapsed":7115,"user":{"displayName":"‍이지현[ 학부재학 / 언어학과 ]","photoUrl":"","userId":"08616836268514229893"}},"outputId":"f970cea2-87a3-4f9c-f0e7-e919fd21345f"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n","\r\u001b[K     |▏                               | 10kB 26.2MB/s eta 0:00:01\r\u001b[K     |▍                               | 20kB 33.6MB/s eta 0:00:01\r\u001b[K     |▋                               | 30kB 22.5MB/s eta 0:00:01\r\u001b[K     |▉                               | 40kB 26.3MB/s eta 0:00:01\r\u001b[K     |█                               | 51kB 25.0MB/s eta 0:00:01\r\u001b[K     |█▎                              | 61kB 27.9MB/s eta 0:00:01\r\u001b[K     |█▌                              | 71kB 17.1MB/s eta 0:00:01\r\u001b[K     |█▊                              | 81kB 18.7MB/s eta 0:00:01\r\u001b[K     |██                              | 92kB 18.1MB/s eta 0:00:01\r\u001b[K     |██▏                             | 102kB 17.9MB/s eta 0:00:01\r\u001b[K     |██▍                             | 112kB 17.9MB/s eta 0:00:01\r\u001b[K     |██▋                             | 122kB 17.9MB/s eta 0:00:01\r\u001b[K     |██▉                             | 133kB 17.9MB/s eta 0:00:01\r\u001b[K     |███                             | 143kB 17.9MB/s eta 0:00:01\r\u001b[K     |███▎                            | 153kB 17.9MB/s eta 0:00:01\r\u001b[K     |███▌                            | 163kB 17.9MB/s eta 0:00:01\r\u001b[K     |███▊                            | 174kB 17.9MB/s eta 0:00:01\r\u001b[K     |████                            | 184kB 17.9MB/s eta 0:00:01\r\u001b[K     |████▏                           | 194kB 17.9MB/s eta 0:00:01\r\u001b[K     |████▎                           | 204kB 17.9MB/s eta 0:00:01\r\u001b[K     |████▌                           | 215kB 17.9MB/s eta 0:00:01\r\u001b[K     |████▊                           | 225kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████                           | 235kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 245kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 256kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 266kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 276kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████                          | 286kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 296kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 307kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 317kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████                         | 327kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 337kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 348kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 358kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 368kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████                        | 378kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 389kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 399kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 409kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 419kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████                       | 430kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 440kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 450kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 460kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████                      | 471kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 481kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 491kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 501kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 512kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████                     | 522kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 532kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 542kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 552kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████                    | 563kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 573kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 583kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 593kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 604kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 614kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 624kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 634kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 645kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 655kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 665kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 675kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 686kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 696kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 706kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 716kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 727kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 737kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 747kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████████                | 757kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 768kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 778kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 788kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 798kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 808kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 819kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 829kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 839kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 849kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 860kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 870kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 880kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 890kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 901kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 911kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 921kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 931kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 942kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 952kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 962kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 972kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 983kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 993kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.0MB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.0MB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.0MB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.0MB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.0MB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.1MB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.1MB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.1MB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.1MB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.1MB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.1MB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.1MB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.1MB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1MB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.1MB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.2MB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.2MB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.2MB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.2MB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.2MB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.2MB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.2MB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2MB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.2MB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.2MB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.3MB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.3MB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.3MB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.3MB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.3MB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.3MB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.3MB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.3MB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.3MB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.4MB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.4MB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.4MB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.4MB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.4MB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.4MB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.4MB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.4MB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.4MB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.4MB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.5MB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.5MB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.5MB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.5MB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.5MB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.5MB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 17.9MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n","Collecting tokenizers==0.9.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 51.0MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 49.2MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=5aeb350b960204bfb90096271fb0a80043b1db9c1ff3f0856d905187a2ae06f9\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.1.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"A3EeF--V9GPO"},"source":["import re\r\n","import numpy as np\r\n","import pandas as pd\r\n","import random\r\n","import time\r\n","import datetime\r\n","from sklearn.preprocessing import LabelEncoder\r\n","from sklearn.model_selection import train_test_split\r\n","from keras.preprocessing.sequence import pad_sequences\r\n","import torch\r\n","from torch import nn\r\n","import torch.nn.functional as F\r\n","import torch.optim as optim\r\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n","from transformers import BertTokenizer\r\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\r\n","from transformers import get_linear_schedule_with_warmup"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uKDqAHj-9zWb"},"source":["train = pd.read_csv(path+ 'news_train.csv')\r\n","test = pd.read_csv(path + 'news_test.csv')\r\n","submission = pd.read_csv(path + 'sample_submission.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"leOB1vnU-zro"},"source":["train['review'] = (train['title'].map(str) +' '+ train['content']).apply(lambda row: row.strip())\r\n","test['review'] = (test['title'].map(str) +' '+ test['content']).apply(lambda row: row.strip())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w86AkMih-0oW"},"source":["# Define the dataset\r\n","X_train = train['review'].values.astype('str')\r\n","y_train = train['info']\r\n","X_test = test['review'].values.astype('str')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"heqFiNrSATxI"},"source":["encoder = LabelEncoder()\r\n","\r\n","encoder = encoder.fit(y_train)\r\n","y_train = encoder.transform(y_train)\r\n","mapping = dict(zip( range(len(encoder.classes_)), encoder.classes_))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["da5ecad17dbb4effa62dcf274ecebde3","4b73658b8cad4e6eabbf75804722e566","871d4ad0d8de4bf480bd7f2ed1036fea","5f0990d7ca0b46d7ac7e91a36f15c10f","27cf473c4fa247769e282dddad9e4ccd","f89d4188df664aff984377d4fcfccc94","31247ce699c2414bb17ceea0732d341c","586bced4b8db4836a6f893622e566ff5"]},"id":"7CVbTK_N-63_","executionInfo":{"status":"ok","timestamp":1609376136744,"user_tz":-540,"elapsed":61564,"user":{"displayName":"‍이지현[ 학부재학 / 언어학과 ]","photoUrl":"","userId":"08616836268514229893"}},"outputId":"010bee06-d7c1-4753-b5f4-f7728d29d492"},"source":["sentences = X_train\r\n","sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\r\n","\r\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\r\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"da5ecad17dbb4effa62dcf274ecebde3","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=995526.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"D8E5OMPj_hb5"},"source":["# 입력 토큰의 최대 시퀀스 길이\r\n","MAX_LEN = 150\r\n","\r\n","# 토큰을 숫자 인덱스로 변환\r\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\r\n","\r\n","# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\r\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ba1k0tfE_jmo"},"source":["# 어텐션 마스크 초기화\r\n","attention_masks = []\r\n","\r\n","# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\r\n","# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\r\n","\r\n","for seq in input_ids:\r\n","    seq_mask = [float(i>0) for i in seq]\r\n","    attention_masks.append(seq_mask)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eDCupedF_uwB"},"source":["# 훈련셋과 검증셋으로 분리\r\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,\r\n","                                                                                    y_train, \r\n","                                                                                    random_state=2018, \r\n","                                                                                    test_size=0.1)\r\n","\r\n","# 어텐션 마스크를 훈련셋과 검증셋으로 분리\r\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, \r\n","                                                       y_train,\r\n","                                                       random_state=2018, \r\n","                                                       test_size=0.1)\r\n","\r\n","# 데이터를 파이토치의 텐서로 변환\r\n","train_inputs = torch.tensor(train_inputs)\r\n","train_labels = torch.tensor(train_labels)\r\n","train_masks = torch.tensor(train_masks)\r\n","validation_inputs = torch.tensor(validation_inputs)\r\n","validation_labels = torch.tensor(validation_labels)\r\n","validation_masks = torch.tensor(validation_masks)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SP-e1aFM_v3Q"},"source":["# 배치 사이즈\r\n","batch_size = 32\r\n","\r\n","# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\r\n","# 학습시 배치 사이즈 만큼 데이터를 가져옴\r\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\r\n","train_sampler = RandomSampler(train_data)\r\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\r\n","\r\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\r\n","validation_sampler = SequentialSampler(validation_data)\r\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BbIPAaIyBPeZ","executionInfo":{"status":"ok","timestamp":1609376155746,"user_tz":-540,"elapsed":71702,"user":{"displayName":"‍이지현[ 학부재학 / 언어학과 ]","photoUrl":"","userId":"08616836268514229893"}},"outputId":"d04ab6fe-b125-4137-cdfc-cc397f8d6627"},"source":["# 디바이스 설정\r\n","if torch.cuda.is_available():    \r\n","    device = torch.device(\"cuda\")\r\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\r\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\r\n","else:\r\n","    device = torch.device(\"cpu\")\r\n","    print('No GPU available, using the CPU instead.')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["988dbe992b8c432b80863cf7e2ea4561","3df5e054bfcc41ad971b428cf2afdeef","aed37a2fc8c64f1a9064bc1b30575525","2de7c80e0d314ba7ac1afc6048bf023e","642a38168ccd401eb02a97631f2ae20f","b9b041ce2cec43a7a9ea761491451a22","f2b4c1018ea94ffa89c8a747b3ac7479","863152ec127641d5acae03ff96e539cb","886e6bd464814a28a7d6d8c977acce82","041cb542ecff4ae6abf1503bc2fefb39","2ade8f6e2a5e4b99a4f387a309fefafd","66b4e1c1cc8a4bbc9bdb60d7efab363a","349ba1e45cf543208edbf92b163050cb","73f03bada66443eba3bb76a417f3cce3","2c0efe2be0b34424a16ef1c2dc6c86c4","f186baa789ae4f49b27e52818617fef0"]},"id":"Nn_u-zkpBB95","executionInfo":{"status":"ok","timestamp":1609376182013,"user_tz":-540,"elapsed":95841,"user":{"displayName":"‍이지현[ 학부재학 / 언어학과 ]","photoUrl":"","userId":"08616836268514229893"}},"outputId":"29da8429-c16b-42ca-fcbb-a63c0926b75d"},"source":["model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels = 2)\r\n","model.cuda()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"988dbe992b8c432b80863cf7e2ea4561","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=625.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"886e6bd464814a28a7d6d8c977acce82","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=714314041.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"vvCMBi2gBFrR"},"source":["# 옵티마이저 설정\r\n","optimizer = AdamW(model.parameters(),\r\n","                  lr = 2e-5, # 학습률\r\n","                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\r\n","                )\r\n","\r\n","# 에폭수\r\n","epochs = 3\r\n","\r\n","# 총 훈련 스텝 : 배치반복 횟수 * 에폭\r\n","total_steps = len(train_dataloader) * epochs\r\n","\r\n","# 처음에 학습률을 조금씩 변화시키는 스케줄러 생성\r\n","scheduler = get_linear_schedule_with_warmup(optimizer, \r\n","                                            num_warmup_steps = 0,\r\n","                                            num_training_steps = total_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A5c8nXI9BKRY"},"source":["# 정확도 계산 함수\r\n","def flat_accuracy(preds, labels):\r\n","    \r\n","    pred_flat = np.argmax(preds, axis=1).flatten()\r\n","    labels_flat = labels.flatten()\r\n","\r\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b-1jR4cHBMVy"},"source":["# 시간 표시 함수\r\n","def format_time(elapsed):\r\n","\r\n","    # 반올림\r\n","    elapsed_rounded = int(round((elapsed)))\r\n","    \r\n","    # hh:mm:ss으로 형태 변경\r\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xV1c9jWWBULh","executionInfo":{"status":"ok","timestamp":1609385592236,"user_tz":-540,"elapsed":9491391,"user":{"displayName":"‍이지현[ 학부재학 / 언어학과 ]","photoUrl":"","userId":"08616836268514229893"}},"outputId":"e1417694-4eb2-4647-aaa6-93e81013e345"},"source":["# 재현을 위해 랜덤시드 고정\r\n","seed_val = 42\r\n","random.seed(seed_val)\r\n","np.random.seed(seed_val)\r\n","torch.manual_seed(seed_val)\r\n","torch.cuda.manual_seed_all(seed_val)\r\n","\r\n","# 그래디언트 초기화\r\n","model.zero_grad()\r\n","\r\n","# 에폭만큼 반복\r\n","for epoch_i in range(0, epochs):\r\n","    \r\n","    # ========================================\r\n","    #               Training\r\n","    # ========================================\r\n","    \r\n","    print(\"\")\r\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\r\n","    print('Training...')\r\n","\r\n","    # 시작 시간 설정\r\n","    t0 = time.time()\r\n","\r\n","    # 로스 초기화\r\n","    total_loss = 0\r\n","\r\n","    # 훈련모드로 변경\r\n","    model.train()\r\n","        \r\n","    # 데이터로더에서 배치만큼 반복하여 가져옴\r\n","    for step, batch in enumerate(train_dataloader):\r\n","        # 경과 정보 표시\r\n","        if step % 500 == 0 and not step == 0:\r\n","            elapsed = format_time(time.time() - t0)\r\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\r\n","\r\n","        # 배치를 GPU에 넣음\r\n","        batch = tuple(t.to(device) for t in batch)\r\n","        \r\n","        # 배치에서 데이터 추출\r\n","        b_input_ids, b_input_mask, b_labels = batch\r\n","\r\n","        # Forward 수행                \r\n","        outputs = model(b_input_ids, \r\n","                        token_type_ids=None, \r\n","                        attention_mask=b_input_mask, \r\n","                        labels=b_labels)\r\n","        \r\n","        # 로스 구함\r\n","        loss = outputs[0]\r\n","\r\n","        # 총 로스 계산\r\n","        total_loss += loss.item()\r\n","\r\n","        # Backward 수행으로 그래디언트 계산\r\n","        loss.backward()\r\n","\r\n","        # 그래디언트 클리핑\r\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n","\r\n","        # 그래디언트를 통해 가중치 파라미터 업데이트\r\n","        optimizer.step()\r\n","\r\n","        # 스케줄러로 학습률 감소\r\n","        scheduler.step()\r\n","\r\n","        # 그래디언트 초기화\r\n","        model.zero_grad()\r\n","\r\n","    # 평균 로스 계산\r\n","    avg_train_loss = total_loss / len(train_dataloader)            \r\n","\r\n","    print(\"\")\r\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\r\n","    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\r\n","        \r\n","    # ========================================\r\n","    #               Validation\r\n","    # ========================================\r\n","\r\n","    print(\"\")\r\n","    print(\"Running Validation...\")\r\n","\r\n","    #시작 시간 설정\r\n","    t0 = time.time()\r\n","\r\n","    # 평가모드로 변경\r\n","    model.eval()\r\n","\r\n","    # 변수 초기화\r\n","    eval_loss, eval_accuracy = 0, 0\r\n","    nb_eval_steps, nb_eval_examples = 0, 0\r\n","\r\n","    # 데이터로더에서 배치만큼 반복하여 가져옴\r\n","    for batch in validation_dataloader:\r\n","        # 배치를 GPU에 넣음\r\n","        batch = tuple(t.to(device) for t in batch)\r\n","        \r\n","        # 배치에서 데이터 추출\r\n","        b_input_ids, b_input_mask, b_labels = batch\r\n","        \r\n","        # 그래디언트 계산 안함\r\n","        with torch.no_grad():     \r\n","            # Forward 수행\r\n","            outputs = model(b_input_ids, \r\n","                            token_type_ids=None, \r\n","                            attention_mask=b_input_mask)\r\n","        \r\n","        # 로스 구함\r\n","        logits = outputs[0]\r\n","\r\n","        # CPU로 데이터 이동\r\n","        logits = logits.detach().cpu().numpy()\r\n","        label_ids = b_labels.to('cpu').numpy()\r\n","        \r\n","        # 출력 로짓과 라벨을 비교하여 정확도 계산\r\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\r\n","        eval_accuracy += tmp_eval_accuracy\r\n","        nb_eval_steps += 1\r\n","\r\n","    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\r\n","    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\r\n","\r\n","print(\"\")\r\n","print(\"Training complete!\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 3 ========\n","Training...\n","  Batch   500  of  3,340.    Elapsed: 0:07:24.\n","  Batch 1,000  of  3,340.    Elapsed: 0:14:57.\n","  Batch 1,500  of  3,340.    Elapsed: 0:22:30.\n","  Batch 2,000  of  3,340.    Elapsed: 0:30:03.\n","  Batch 2,500  of  3,340.    Elapsed: 0:37:36.\n","  Batch 3,000  of  3,340.    Elapsed: 0:45:09.\n","\n","  Average training loss: 0.06\n","  Training epcoh took: 0:50:16\n","\n","Running Validation...\n","  Accuracy: 0.99\n","  Validation took: 0:02:01\n","\n","======== Epoch 2 / 3 ========\n","Training...\n","  Batch   500  of  3,340.    Elapsed: 0:07:32.\n","  Batch 1,000  of  3,340.    Elapsed: 0:15:04.\n","  Batch 1,500  of  3,340.    Elapsed: 0:22:36.\n","  Batch 2,000  of  3,340.    Elapsed: 0:30:08.\n","  Batch 2,500  of  3,340.    Elapsed: 0:37:39.\n","  Batch 3,000  of  3,340.    Elapsed: 0:45:11.\n","\n","  Average training loss: 0.02\n","  Training epcoh took: 0:50:18\n","\n","Running Validation...\n","  Accuracy: 0.99\n","  Validation took: 0:02:01\n","\n","======== Epoch 3 / 3 ========\n","Training...\n","  Batch   500  of  3,340.    Elapsed: 0:07:31.\n","  Batch 1,000  of  3,340.    Elapsed: 0:15:02.\n","  Batch 1,500  of  3,340.    Elapsed: 0:22:34.\n","  Batch 2,000  of  3,340.    Elapsed: 0:30:05.\n","  Batch 2,500  of  3,340.    Elapsed: 0:37:36.\n","  Batch 3,000  of  3,340.    Elapsed: 0:45:07.\n","\n","  Average training loss: 0.01\n","  Training epcoh took: 0:50:14\n","\n","Running Validation...\n","  Accuracy: 1.00\n","  Validation took: 0:02:01\n","\n","Training complete!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EeuhcyIx3kid"},"source":["# Test data에 적용 (Predict)\r\n","test_sentences = X_test\r\n","\r\n","# BERT의 토크나이저로 문장을 토큰으로 분리\r\n","test_tokenized_texts = [tokenizer.tokenize(sent) for sent in test_sentences]\r\n","\r\n","# 토큰을 숫자 인덱스로 변환\r\n","test_input_ids = [tokenizer.convert_tokens_to_ids(x) for x in test_tokenized_texts]\r\n","\r\n","# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\r\n","test_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\r\n","\r\n","# 어텐션 마스크 초기화\r\n","test_attention_masks = []\r\n","\r\n","# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\r\n","# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\r\n","for seq in test_input_ids:\r\n","    seq_mask = [float(i>0) for i in seq]\r\n","    test_attention_masks.append(seq_mask)\r\n","\r\n","# 데이터를 파이토치의 텐서로 변환\r\n","test_inputs = torch.tensor(test_input_ids)\r\n","test_masks = torch.tensor(test_attention_masks)\r\n","\r\n","test_data = TensorDataset(test_inputs, test_masks)\r\n","test_sampler = RandomSampler(test_data)\r\n","test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GFJe9tgF3jXf","executionInfo":{"status":"ok","timestamp":1609390366365,"user_tz":-540,"elapsed":1449368,"user":{"displayName":"‍이지현[ 학부재학 / 언어학과 ]","photoUrl":"","userId":"08616836268514229893"}},"outputId":"9b7a679d-5551-47f6-d613-c6a70f67ae7b"},"source":["print('Predicting labels for {:,} test sentences...'.format(len(test_input_ids)))\r\n","\r\n","# 평가모드로 변경\r\n","model.eval()\r\n","\r\n","# predictions\r\n","predictions = []\r\n","\r\n","# 데이터로더에서 배치만큼 반복하여 가져옴\r\n","for batch in test_dataloader:\r\n","\r\n","    # 배치를 GPU에 넣음\r\n","    batch = tuple(t.to(device) for t in batch)\r\n","    \r\n","    # 배치에서 데이터 추출\r\n","    b_input_ids, b_input_mask = batch\r\n","    \r\n","    # 그래디언트 계산 안함\r\n","    with torch.no_grad():     \r\n","        # Forward 수행\r\n","        outputs = model(b_input_ids, \r\n","                        token_type_ids=None, \r\n","                        attention_mask=b_input_mask)\r\n","    \r\n","    # 로스 구함\r\n","    logits = outputs[0]\r\n","\r\n","    # CPU로 데이터 이동\r\n","    logits = logits.detach().cpu().numpy()\r\n","\r\n","    # predictions 결과 저장\r\n","    predictions.append(logits)\r\n","\r\n","print(\"Done\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Predicting labels for 142,565 test sentences...\n","Done\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"v212bQErDp7a"},"source":["# 모든 배치의 결과를 combine\r\n","flat_predictions = np.concatenate(predictions, axis=0)\r\n","\r\n","# 각 샘플마다, 높은 점수를 가진 label (0 or 1) 선택\r\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\r\n","\r\n","# Main dataframe에 추가\r\n","test['target'] = flat_predictions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QdhdQz2QElh5"},"source":["df = pd.DataFrame({'id': test['id'], 'info': test['target']})\r\n","df.to_csv(path + \"sample_submission_bert.csv\", mode='w', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rL90y0qRKmII"},"source":["https://github.com/huggingface/transformers/blob/35ff345fc9df9e777b27903f11fa213e4052595b/examples/run_glue.py#L495"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wMkCLPlBJYKe","executionInfo":{"status":"ok","timestamp":1609391170923,"user_tz":-540,"elapsed":5176,"user":{"displayName":"‍이지현[ 학부재학 / 언어학과 ]","photoUrl":"","userId":"08616836268514229893"}},"outputId":"f4cb5e89-0468-400b-c2c4-99aad86dfef4"},"source":["# 모델 저장\r\n","import os\r\n","\r\n","output_dir = path\r\n","\r\n","# Create output directory if needed\r\n","if not os.path.exists(output_dir):\r\n","    os.makedirs(output_dir)\r\n","\r\n","print(\"Saving model to %s\" % output_dir)\r\n","\r\n","# Save a trained model, configuration and tokenizer using `save_pretrained()`.\r\n","# They can then be reloaded using `from_pretrained()`\r\n","model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\r\n","model_to_save.save_pretrained(output_dir)\r\n","tokenizer.save_pretrained(output_dir)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Saving model to /content/drive/MyDrive/Colab Notebooks/Dacon/NH투자/data/\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["('/content/drive/MyDrive/Colab Notebooks/Dacon/NH투자/data/tokenizer_config.json',\n"," '/content/drive/MyDrive/Colab Notebooks/Dacon/NH투자/data/special_tokens_map.json',\n"," '/content/drive/MyDrive/Colab Notebooks/Dacon/NH투자/data/vocab.txt',\n"," '/content/drive/MyDrive/Colab Notebooks/Dacon/NH투자/data/added_tokens.json')"]},"metadata":{"tags":[]},"execution_count":44}]}]}